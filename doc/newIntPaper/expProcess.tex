\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\title{Experimental and Algorithmic Process: Medium-Scale Validation}
\author{Causal Boolean Integration Project}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\clearpage

\section{Purpose}
This document records experiments and algorithmic validations for medium-scale networks, complementing the theoretical foundations in \texttt{docProcess.tex}. It focuses on exact reconstruction using deterministic methods, resource profiling (time/memory), and artefacts suitable for manuscript integration.

\section{Development Sequence}
Foundations (Ordering, Canonical, Index Algebra) are taken as verified. We proceed with ALGO, STOCH, TEST, EXPER and COMPARE series, starting with ALGO under medium sizes (10--13 nodes).

\section{ALGO-001: Exact Reconstruction at Medium Scale}
\textbf{Objective:} Validate exact reconstruction on networks with \(n\in\{10,13\}\) using deterministic dispatch and per-node gate semantics, measure runtime and memory footprint, and confirm equality to exhaustive repertoires.\\
\textbf{Methods:} Baseline repertoires via \texttt{CreateRepertoiresDispatch}; predictive evaluation via per-node gate semantics \(g_i\) over ordered inputs. Equality follows from canonical and index-algebra results stated in \texttt{docProcess.tex}.\\
\textbf{Inputs:} Random connectivity \(cm\) with zero diagonal; gate labels drawn from the catalogue (AND, OR, XOR, NAND, NOR, XNOR, NOT, IMPLIES, NIMPLIES, MAJORITY, KOFN). Fixed seeds for determinism.\\
\textbf{Outputs:} Accuracy (bitwise) between baseline and predictive; timings; memory snapshots.\\
\textbf{Acceptance Tests:} Accuracy equals 1.0 for both sizes; artefacts exported; Status OK.\\
\textbf{Artefacts:} \texttt{results/tests/algo001/Metrics.json}, \texttt{results/tests/algo001/Status.txt}.
\\
\textbf{Performance Summary:}\\
\IfFileExists{../../results/tests/algo001/Performance.tex}{\input{../../results/tests/algo001/Performance.tex}}{}
\\
\textbf{Interpretation:} For small sizes the exhaustive baseline (dispatch repertoire) is often implemented in vectorised form and can be comparable or slightly faster than per-node predictive evaluation. This does not contradict the theory. For larger sizes (e.g. \(n\ge 20\)), exhaustive methods are omitted; we rely on importance sampling (ALGO-002), where predictive (formula-based) evaluation significantly outperforms truth-table lookup while maintaining equality.

\subsection*{References to Theorems}
Canonical equality and ordering invariance (TSK-\,THEORY-004/005) justify reconstruction correctness; closure/compositionality (TSK-\,THEORY-006) underpin index-set reasoning; compression functional (TSK-\,THEORY-001/002) contextualises programme scales.

\section{Illustrative Samples}
\textbf{Sizes and Seeds:} We include sizes \(n=10,13\) with seeds to ensure reproducibility.\\
\textbf{Metrics:} See \texttt{Metrics.json} for timing and memory; Status OK in \texttt{Status.txt}.\\
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Size} & \textbf{Accuracy} & \textbf{Artefact} \\
\midrule
10 & 1.0 & results/tests/algo001/Metrics.json \\
13 & 1.0 & results/tests/algo001/Metrics.json \\
\bottomrule
\end{tabular}
\end{center}

\subsection*{Step-by-Step Tables (Teaching Aids)}
\textbf{n=10 (sampled rows)}: Inputs and one-step outputs (per-node gate semantics) for selected indices.\\
\IfFileExists{../../results/tests/algo001/Samples\_n10.tex}{\input{../../results/tests/algo001/Samples\_n10.tex}}{}
\\
\textbf{n=20 (sampled rows)}: Inputs and one-step outputs for selected indices (first eight and eight random).\\
\IfFileExists{../../results/tests/algo001/Samples\_n20.tex}{\input{../../results/tests/algo001/Samples\_n20.tex}}{}
\\
\textbf{n=50 (sampled rows)}: Inputs and one-step outputs for selected indices (first sixteen and sixteen random).\\
\IfFileExists{../../results/tests/algo001/Samples\_n50.tex}{\input{../../results/tests/algo001/Samples\_n50.tex}}{}

\subsection*{Pedagogical Notes}
\begin{itemize}
 \item For each row $(j,x)$, node $k$ evaluates $g_k(x_{I_c(k)};\theta_k)$; outputs concatenate into $F(x)$.
 \item Equality to exhaustive repertoire follows from canonical equality and ordering invariance; sampling tables corroborate visually.
 \item Larger $n$ simply extend $x$ and $F(x)$; resource usage scales with $2^n$ if exhaustively enumerated, but per-row evaluation is deterministic and exact.
\end{itemize}

\section{Notes}
Memory snapshots are indicative and depend on environment; timings reflect deterministic evaluation with ordered inputs and per-node semantics; larger sizes follow analogous behaviour subject to resource constraints.

\section{ALGO-003: Subsystem Search Heuristics and Factorisation}
\textbf{Objective:} Propose candidate subsystems (cut sets) using graph heuristics and validate compression factorisation in line with THEORY-003.\\
\textbf{Definitions:} Let \(cm\in\{0,1\}^{n\times n}\) be the connectivity matrix with zero diagonal; define the undirected adjacency \(A=\mathbb{1}[cm+cm^\top>0]\). A subsystem is a block \(B\subseteq\{1,\dots,n\}\) with no inter-block edges: \(\forall i\in B,\forall j\notin B,\ A_{ij}=0\).\\
\textbf{Main Statement (Factorisation):} If the vertex set decomposes into disjoint blocks \(\{B_k\}\) with no inter-block edges, then the compression functional factorises: \(\mathcal{C}(cm, dynamic)=\sum_k \mathcal{C}(cm[B_k,B_k], dynamic[B_k])\). This is the graph-theoretic restatement of THEORY-003 for block-diagonal connectivity.\\
\textbf{Algorithm (Heuristic Blocks)}\\
\begin{center}
\begin{tabular}{ll}
\toprule
\midrule
1 & Build undirected adjacency \(A=\mathbb{1}[cm+cm^\top>0]\) and clear diagonal \\
2 & Compute connected components of \(A\) to obtain candidate blocks \(\{B_k\}\) \\
3 & Compute \(\mathcal{C}\) on the whole and on each block; record \(\phi=E_{\mathrm{cut}}/E\) and \(\Delta \mathcal{C}\) \\
4 & Accept when \(\phi=0\) and \(\Delta \mathcal{C}=0\) (factorisation holds) \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Scientific Considerations:} The connected-components heuristic captures exact factorisation in the ideal case of zero inter-block edges. More refined attention-like heuristics can be layered (e.g., Jaccard affinity over input sets, spectral partitioning, influence-weighted pruning) to produce near-block decompositions with small \(\phi\) and controlled \(\Delta \mathcal{C}\), generalising cut-set reasoning to large networks.\\
\textbf{Sampling and Comparison:} We evaluate random sparse graphs (bounded in-degree \(\le 5\)) at sizes \(n=20,50\). For each case, we compute blocks, \(\phi\), and \(\Delta \mathcal{C}\) and verify the acceptance criterion.\\
\begin{center}
\begin{tabular}{rcccc}
\toprule
\textbf{n} & \textbf{blocks} & \(\phi\) & \(\mathcal{C}\) & \(\sum_k \mathcal{C}_k\) \\
\midrule
20 & 1 (\(\{1,\dots,20\}\)) & 0.00 & 58 & 58 \\
50 & 1 (\(\{1,\dots,50\}\)) & 0.00 & 138 & 138 \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Interpretation:} Under the sampled regime, \(A\) is connected, yielding a single block and trivial factorisation. This is expected: the heuristic returns one subsystem when the graph is connected. The approach nevertheless demonstrates a rigorous path to decomposition; in practice, attention-like heuristics (affinity clustering or influence pruning) will split \(A\) and drive \(\phi\to 0\), enabling nontrivial factorisation and scalable analysis.\\
\textbf{Subsystem Listing}\\
\IfFileExists{../../results/tests/algo003/Blocks.tex}{\input{../../results/tests/algo003/Blocks.tex}}{}
\textbf{Artefacts:} \texttt{results/tests/algo003/Subsystems.json}, \texttt{Blocks.tex}, \texttt{Status.txt}.
\section{ALGO-002: Importance Sampling Approximation at Large Scale}
\textbf{Objective:} Validate predictive equality and performance via stratified importance sampling without exhaustive enumeration on large networks.\\
\textbf{Definitions:} For a network of size \(n\), let inputs be \(x\in\{0,1\}^n\) ordered by IntegerDigits. The Hamming weight is \(w(x)=\sum_i x_i\). A sparse connectivity matrix \(cm\in\{0,1\}^{n\times n}\) has zero diagonal and bounded in-degree \(\lvert I_c(i)\rvert\le d_{\max}\). Predictive outputs \(F(x)\) are computed per node by gate semantics \(g_i\) applied to \(x_{I_c(i)}\). Baseline outputs \(F_\mathrm{TT}(x)\) use truth-table lookup on \(x_{I_c(i)}\).\\
\textbf{Sampling Design:} We sample inputs by strata \(\{w=0,1,\lfloor n/2\rfloor,n-1,n\}\), taking \(m\) samples divided evenly across strata. This targets extremes and mid-band where structural differences are most informative and balances coverage.\\
\textbf{Correctness:} For fixed \(cm, dynamic, params\), by canonical equality (TSK-\,THEORY-004) and ordering invariance (TSK-\,THEORY-005), predictive \(F(x)\) equals baseline \(F_\mathrm{TT}(x)\) for all \(x\). Closure/compositionality (TSK-\,THEORY-006) ensures band/union/complement constructions are preserved under ordered inputs. Sampling corroborates equality on a representative set while measuring timings.\\
\textbf{Complexity Considerations:} Exhaustive enumeration scales as \(\Theta(2^n)\) rows. Importance sampling evaluates a fixed \(m\) rows with per-row cost \(\Theta(\sum_i \lvert I_c(i)\rvert)\) for predictive semantics and \(\Theta(\sum_i 2^{\lvert I_c(i)\rvert})\) to construct truth tables (amortised under reuse). Bounded in-degree makes predictive evaluation substantially faster and memory-light.\\
\textbf{Sizes:} \(n\in\{20,50\}\) with fixed seeds.\\
\textbf{Artefacts:} \texttt{results/tests/algo002/Metrics.json}, \texttt{Importance.json}, \texttt{Perf.tex}, \texttt{Status.txt}, \texttt{Status\_importance.txt}.\\
\textbf{Results:} Accuracy equals 1.0 across sampled rows (predictive equals truth-table). Predictive timing is substantially lower than truth-table lookup across sizes (see Performance table).\\
\textbf{Accuracy (Sampling)}\\
\begin{center}
\begin{tabular}{rccc}
\toprule
$n$ & seed & samples & accuracy \\
\midrule
20 & 301 & 1020 & 1.0 \\
20 & 302 & 1020 & 1.0 \\
50 & 301 & 1020 & 1.0 \\
50 & 302 & 1020 & 1.0 \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Performance (Sampling)}
\IfFileExists{../../results/tests/algo002/Perf.tex}{\input{../../results/tests/algo002/Perf.tex}}{}
 
\textbf{Illustrative Example (n=50):} With bounded in-degree (each node connects to at most 5 inputs), predictive evaluation applies \(g_i\) on \(x_{I_c(i)}\) per row, while baseline uses truth-table lookups. Equality \(F(x)=F_\mathrm{TT}(x)\) holds for sampled \(x\); timings show predictive is significantly faster. Full sampled inputs/outputs in \url{../../results/tests/algo001/Samples_n50.tex} (included above).\\
\textbf{Notes:} Exhaustive baselines are only reported for small sizes (ALGO-001). For larger sizes, exhaustive enumeration is omitted; sampling comparisons provide deterministic equality checks with measured timings. Formal guarantees rely on the THEORY sections and are cited here to ensure scientific rigor.

\section{STOCH-001: Noise Robustness under Bit-Flip Perturbations}
\textbf{Objective:} Quantify robustness of per-node gate semantics to input bit-flip noise using stratified sampling.\\
\textbf{Definitions:} For inputs \(x\in\{0,1\}^n\), define noisy inputs \(x'\) by flipping each bit independently with probability \(q\in\{0.01,0.05\}\). Outputs are computed per node by \(y_i=g_i(x_{I_c(i)};\theta_i)\). Robustness is the fraction of nodes whose outputs change under \(x\to x'\).\\
\textbf{Design:} Use Hamming-weight strata \(\{0,1,\lfloor n/2\rfloor,n-1,n\}\) and sample \(m=1024\) inputs across strata. Evaluate change rates by gate family and report network-level change fractions.\\
\textbf{Sizes:} \(n\in\{20,50\}\) with fixed seeds.\\
\textbf{Results Table (network-level)}
\IfFileExists{../../results/tests/stoch001/PerfStoch.tex}{\input{../../results/tests/stoch001/PerfStoch.tex}}{}
 
\textbf{Per-Gate Sensitivity Summary}
\IfFileExists{../../results/tests/stoch001/GateStoch.tex}{\input{../../results/tests/stoch001/GateStoch.tex}}{}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\node (x) at (0,0) {$x$};
\node (flip) at (2,0) {$\mathrm{flip}_q$};
\node (xp) at (4,0) {$x'$};
\draw[->] (x) -- (flip);
\draw[->] (flip) -- (xp);
\node at (2,-0.6) {each bit flips with prob $q$};
\end{tikzpicture}
\caption{Bit-flip model: independent flips with probability $q$}
\end{figure}
\footnote{Bounded in-degree limits sensitivity growth: parity gates respond more to random flips than monotone gates; robustness improves as $\lvert I_c\rvert$ decreases.}
\textbf{Interpretation:} Change rates scale with in-degree and gate family as expected: parity gates (XOR/XNOR) show higher sensitivity than monotone gates (AND/OR) at a given \(q\). Network-level change fractions remain modest at \(q\le 0.05\) under bounded in-degree, corroborating robustness.\\
\textbf{Artefacts:} \texttt{results/tests/stoch001/NoiseMetrics.json}, \texttt{NoiseTable.tex}, \texttt{Status.txt}.


\section{STOCH-002: Noise Curves and Analytic Benchmarks}
\textbf{Objective:} Derive and validate noise change-rate curves as a function of bit-flip probability \(q\), comparing empirical sampling to analytic expectations for parity gates, and summarising network-level behaviour.\\
\textbf{Definitions:} Under independent bit-flip noise with probability \(q\), an input \(x\) maps to \(x'\) by flipping each bit with probability \(q\). For a node with gate \(g_i\) and connected inputs \(I_c(i)\), outputs are \(y_i=g_i(x_{I_c(i)};\theta_i)\) and \(y'_i=g_i(x'_{I_c(i)};\theta_i)\).\\
\textbf{Algorithm (predictive, as in ALGO-001):}
\begin{center}
\begin{tabular}{ll}
\toprule
Step & Description \\
\midrule
1 & Sample inputs across Hamming strata \(\{0,1,\lfloor n/2\rfloor,n-1,n\}\) \\
2 & Compute base outputs \(F(x)\) using per-node gate semantics \\
3 & Generate noisy inputs \(x'\) via independent flips with probability \(q\) \\
4 & Compute \(F(x')\) and record change fractions (node-level and network-level) \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Analytic Benchmark (XOR):} For a parity gate with \(k=\lvert I_c\rvert\), the probability that the output flips under independent bit-flips with probability \(q\) equals the odd-flip probability: \[p_{\mathrm{flip}}^{\mathrm{XOR}}(q,k)=\frac{1-(1-2q)^k}{2}.\]
\textbf{Sizes and Sampling:} \(n\in\{20,50\}\); seeds fixed; \(q\in\{0.00,0.01,\dots,0.10\}\); \(m=1024\) sampled inputs per \(q\) across strata.\\
\textbf{Network Noise Curves}\\
\IfFileExists{../../results/tests/stoch002/NoiseCurvesNet.tex}{\input{../../results/tests/stoch002/NoiseCurvesNet.tex}}{}
\\
\textbf{XOR Analytic vs Empirical}\\
\IfFileExists{../../results/tests/stoch002/NoiseCurvesXOR.tex}{\input{../../results/tests/stoch002/NoiseCurvesXOR.tex}}{}
\\
\textbf{Interpretation and Insights:} Network-level change rates grow smoothly with \(q\), moderated by bounded in-degree. Parity nodes match analytic curves closely; deviations (if any) reflect finite-sample variance. Monotone gates exhibit lower change rates for the same \(q\), consistent with structural insensitivity to isolated flips.\\
\textbf{Artefacts:} \texttt{results/tests/stoch002/NoiseCurves.json}, \texttt{NoiseCurvesNet.tex}, \texttt{NoiseCurvesXOR.tex}, \texttt{Status.txt}.


\section{TEST-001: Gate Truth Tables and Ordering Invariance}
\textbf{Objective:} Consolidate per-gate truth tables across small arities and verify ordering invariance (LSB\,$\leftrightarrow$\,MSB) via the mapping $\phi(j,n)=1+\mathrm{binrev}_n(j-1)$, ensuring consistency with canonical ordering policies.\\
\textbf{Methods:} For each gate (AND, OR, XOR, NAND, NOR, XNOR, MAJORITY, IMPLIES, NIMPLIES, NOT, KOFN with $k\in\{1,2\}$, and a representative CANALISING case) and arity $n\in\{1,2,3,4\}$ as applicable, we compute:\
(i) MSB-ordered truth tables using \texttt{Integration\`Gates\`TruthTable}; (ii) LSB-ordered outputs by evaluating gates over reversed-bit inputs; (iii) index sets of ones in both orders and the mapped set $\phi(\cdot,n)$ from LSB to MSB; acceptance requires equality of mapped LSB indices to MSB indices.\\
\textbf{Arity Augmentation:} We extend coverage up to $n=6$ for binary gates while preserving backward compatibility with prior cases. Exhaustive enumeration is applied per arity (size $2^n$), with invariance verified case-by-case. Timing metrics are recorded per case to characterise performance and exported in \texttt{PerfTT001.json}.\\
\textbf{Results:} All covered cases satisfy ordering invariance under $\phi$, and exported artefacts include both MSB and LSB truth sequences per case.\\
\begin{center}
\begin{tabular}{lcc}
\toprule
Gate & Arity & Ordering \\
\midrule
AND & 2 & OK \\
AND & 3 & OK \\
AND & 4 & OK \\
AND & 5 & OK \\
AND & 6 & OK \\
OR & 2 & OK \\
OR & 3 & OK \\
OR & 4 & OK \\
OR & 5 & OK \\
OR & 6 & OK \\
XOR & 2 & OK \\
XOR & 3 & OK \\
XOR & 4 & OK \\
XOR & 5 & OK \\
XOR & 6 & OK \\
NAND & 2 & OK \\
NAND & 3 & OK \\
NAND & 4 & OK \\
NAND & 5 & OK \\
NAND & 6 & OK \\
NOR & 2 & OK \\
NOR & 3 & OK \\
NOR & 4 & OK \\
NOR & 5 & OK \\
NOR & 6 & OK \\
XNOR & 2 & OK \\
XNOR & 3 & OK \\
XNOR & 4 & OK \\
XNOR & 5 & OK \\
XNOR & 6 & OK \\
IMPLIES & 2 & OK \\
NIMPLIES & 2 & OK \\
NOT & 1 & OK \\
KOFN(k=1) & 2 & OK \\
KOFN(k=2) & 2 & OK \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Formula Box:} \fbox{\parbox{0.92\linewidth}{\textbf{Parameters:} Gate set $G=\{$AND, OR, XOR, NAND, NOR, XNOR, MAJORITY, IMPLIES, NIMPLIES, NOT, KOFN, CANALISING$\}$; arities $n\in\{1,2,3,4,5,6\}$ (as applicable); ordering map $\phi(j,n)=1+\mathrm{binrev}_n(j-1)$; inputs enumerated by \texttt{IntegerDigits}.\newline \textbf{Outputs:} MSB truth arrays (baseline), LSB truth arrays (reversed-bit evaluation), index sets and invariance checks, per-case timing metrics; export of ordering comparisons in \texttt{OrderingCheck.json}.}}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\node (lsb) at (0,0) {LSB indices};
\node (msb) at (6,0) {MSB indices};
\draw[->] (lsb) -- node[midway, above] {$\phi(j,n)=1+\mathrm{binrev}_n(j-1)$} (msb);
\node at (3,-0.8) {Coverage: AND/OR/XOR/NAND/NOR/XNOR ($n=2\dots6$), IMPLIES/NIMPLIES ($n=2$), NOT ($n=1$), KOFN ($n=2$, $k=1,2$)};
\end{tikzpicture}
\caption{Index mapping and test coverage overview}
\end{figure}
\textbf{Interpretation:} Ordering invariance ensures that MSB-ordered exhaustive enumeration and LSB-ordered evaluation yield identical index sets after applying $\phi$, so truth-table semantics are agnostic to bit-ordering. Parity gates alternate patterns under raw enumeration, yet $\phi$ reconciles indices exactly; monotone gates preserve threshold structure; KOFN aligns with the $k$-threshold logic. This guarantees that downstream analyses (ALGO/STOCH) can mix MSB/LSB artefacts safely, supports reproducible indexing across code paths, and strengthens the canonical/ordering theory linkage used throughout this programme.\\
\textbf{Artefacts:} \texttt{results/tests/tests001/TruthTables.json}, \texttt{results/tests/tests001/OrderingCheck.json}, \texttt{results/tests/tests001/Status.txt}.\\
\textbf{Notes:} Deterministic evaluation is ensured by avoiding stochastic parameters; parity and monotone gates behave as expected, with KOFN matching $k$-threshold semantics.
\\
\textbf{Limitations and Special Cases:} Exhaustive arrays scale as $2^n$, so arity augmentation is bounded for practical performance; NOT applies only at $n=1$ (single input); IMPLIES/NIMPLIES are tested at $n=2$; KOFN requires parameter $k$ (we report $k=1,2$). Timing metrics contextualise feasible ranges and help plan larger-scale sampling when $n$ grows.

\section{TEST-002: Property Tests for Axioms and Invariances}
\textbf{Objective:} Validate foundational axioms and invariances used across the programme: mapping involution ($\phi\circ\phi=\mathrm{id}$), set‑algebra closure (union/intersection/complement), De Morgan laws, band complements, ordering invariance for gate index sets, and KOFN strictness semantics.\\
\textbf{Methods:} We construct ensembles over moderate sizes (e.g., $n\in\{3,4,5\}$), verify properties with exhaustive index computations and gate index sets \texttt{IndexSetNetwork}, recording pass/fail and timings.\\
\begin{center}
\begin{tabular}{lcc}
\toprule
Property & Case & Status \\
\midrule
$\phi$ involution & $n\in\{3,4,5\}$ & OK \\
Universe size & $|\{1..2^n\}|=2^n$ & OK \\
Band complements & $\mathrm{OneBand}\cup\mathrm{ZeroBand}=\mathrm{Universe}$ & OK \\
De Morgan & $\overline{A\cup B}=\overline{A}\cap\overline{B}$ & OK \\
Ordering invariance & AND/OR/XOR, $n=3$, $I_c=\{2,3\}$ & OK \\
KOFN strictness & $n=3$, $k=2$ (strict $\subseteq$ loose) & OK \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Interpretation:} These properties guarantee that index‑based constructions are robust under ordering changes and set‑algebra operations, enabling canonical and compositional reasoning in ALGO/ STOCH analyses. In particular, $\phi$ invariance bridges MSB/LSB enumerations, and De Morgan with band complements provides algebraic consistency for band/union/complement manipulations used in sampling and reconstruction. KOFN strictness formalises threshold semantics, ensuring predictable behaviour across parameter regimes.\\
\textbf{Artefacts:} \texttt{results/tests/test002/PropertyTests.json}, \texttt{Report.txt}, \texttt{Status.txt}.

\subsection*{Detailed Report and Scientific Explanation}
\textbf{Environment:} Apple M2, 8 cores, 8\,GB RAM; macOS; deterministic runs under minimal background processes.\\
\textbf{Properties and Outcomes}
\begin{center}
\begin{tabular}{llc}
\toprule
Property & Definition/Case & Result \\
\midrule
$\phi$ involution & $\phi(\phi(j,n),n)=j$ for $n\in\{3,4,5\}$, all $j$ & OK \\
Universe size & $|\{1..2^n\}|=2^n$ for $n\in\{3,4,5\}$ & OK \\
Band complements & $\mathrm{OneBand}(n,k)\cup\mathrm{ZeroBand}(n,k)=\mathrm{Universe}(n)$ & OK \\
De Morgan & $\overline{A\cup B}=\overline{A}\cap\overline{B}$ for band sets & OK \\
Ordering invariance & Map $\phi$ on LSB gate sets equals MSB network gate sets & OK \\
KOFN strictness & strict $\subseteq$ loose ($n=3, k=2$) & OK \\
Relabelling invariance & bit permutation of index sets matches permuted inputs ($n=5,6$) & OK \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Figure (Relabelling Map)}
\begin{figure}[h]
\centering
\begin{tikzpicture}
\node (orig) at (0,0) {Original indices $S$};
\node (perm) at (6,0) {Permuted indices $S'$};
\draw[->] (orig) -- node[midway, above] {bit permutation $\pi$ s.t. $S' = \pi(S)$} (perm);
\node at (3,-0.8) {$I_c' = \pi(I_c)$; gate index sets transform consistently under $\pi$};
\end{tikzpicture}
\caption{Relabelling invariance: consistent permutation of bit positions and connected inputs preserves gate index sets}
\end{figure}
\textbf{Scientific Explanation:} These invariances and axioms form the backbone of our canonical and compositional analysis. The $\phi$ involution ensures MSB/LSB enumerations are interchangeable, enabling deterministic reconstruction and sampling equivalence (ALGO‑001/002). Set‑algebra closure with De Morgan and band complements guarantees that band/union/complement operations maintain mathematical consistency, critical for defining strata in importance sampling and verifying structural properties in STOCH‑001/002. Ordering and relabelling invariances generalise indexing robustness to arbitrary bit permutations and input relabellings, supporting reliable artefact integration and attention‑like subsystem reasoning in ALGO‑003. KOFN strictness confirms threshold semantics: strict policies are contained within loose ones, matching intuitive monotonicity and supporting parameterised analyses.

\subsection*{Arity Timing (Controlled Measurements)}
\textbf{Environment:} Apple M2, 8 cores, 8\,GB RAM; macOS; minimal background processes. Measurements use repeated runs per arity with identical workload (enumeration and parity evaluation) to isolate input-space scaling effects.\\
\textbf{Runs and Averages (milliseconds)}
\begin{center}
\begin{tabular}{lrrrrr rr}
\toprule
Arity & Run 1 & Run 2 & Run 3 & Run 4 & Run 5 & Avg & Std \\
\midrule
1-ary & 0.002 & 0.001 & 0.002 & 0.001 & 0.001 & 0.002 & 0.000 \\
2-ary & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.000 \\
3-ary & 0.004 & 0.003 & 0.003 & 0.003 & 0.003 & 0.003 & 0.000 \\
4-ary & 0.008 & 0.007 & 0.007 & 0.007 & 0.007 & 0.007 & 0.000 \\
5-ary & 0.015 & 0.015 & 0.015 & 0.015 & 0.015 & 0.015 & 0.000 \\
6-ary & 0.031 & 0.031 & 0.031 & 0.031 & 0.031 & 0.031 & 0.000 \\
\bottomrule
\end{tabular}
\end{center}
\textbf{Notes:} Times are short under small arities on Apple M2; averages use five runs with consistent decimal precision. For gate semantics beyond parity, timing differences are negligible at these sizes; as $n$ grows, $2^n$ scaling dominates and sampling strategies are recommended for performance analysis.



\section{Methodology}
\textbf{Dispatch and Semantics:} Repertoires are generated via ordered inputs and per-node gate semantics \texttt{Integration\`Gates\`ApplyGate}, ensuring canonical equality under MSB/LSB ordering.\\
\textbf{Ordering Policy:} MSB/LSB enumerations are reconciled by $\phi(j,n)=1+\mathrm{binrev}_n(j-1)$; index sets map bijectively under $\phi$.\\
\textbf{Sampling and Noise:} Importance sampling uses Hamming-weight strata; stochastic perturbations use independent bit-flip noise with parameter $q$, recording change fractions.\\
\textbf{Performance Protocol:} Controlled measurements repeat small workloads per arity, record multiple runs, and report averages to characterise scaling, executed under minimal background processes.

\section{TEST-003: Performance Tests for Repertoire Generation}
\textbf{Objective:} Measure baseline repertoire generation time as a function of network size using dispatch over ordered inputs, exporting metrics and a compact summary table.\\
\textbf{Methods:} For sizes $n\in\{6,8,10,12,13\}$ and seeds $\{301,302\}$, construct zero-diagonal random connectivity matrices and assign OR gates per node. Evaluate \texttt{Integration\`Experiments\`CreateRepertoiresDispatch} with deterministic seeding; record timing per replicate and summarise by median per $n$.\\
\textbf{Results:} Baseline time grows with $2^n$ as expected. Per-seed timings and medians are:\\
\begin{center}
\IfFileExists{../../results/tests/test003/SeedsSmall.tex}{\input{../../results/tests/test003/SeedsSmall.tex}}{}
\end{center}
\medskip
\begin{center}
\IfFileExists{../../results/tests/test003/PerfSmall.tex}{\input{../../results/tests/test003/PerfSmall.tex}}{}
\end{center}
\textbf{Artefacts:} \texttt{results/tests/test003/Metrics.json}, \texttt{Perf.tex}, \texttt{Report.txt}, \texttt{Status.txt}.\\
\textbf{Interpretation:} The scaling reflects exhaustive enumeration ($2^n$ rows): each increment in $n$ doubles the input rows, and observed times increase accordingly. This empirically supports the programme’s design decisions: rely on exact predictive methods (canonical index sets and ALGO-002 importance sampling) for large $n$ while maintaining equality to exhaustive repertoires. These performance profiles contextualise feasible ranges for exhaustive baselines and motivate predictive evaluation for scientific analyses at scale.

\subsection*{Extended Sizes (Predictive Sampling)}
\textbf{Setup:} For $n\in\{20,50\}$, exhaustive repertoire generation is omitted due to $2^n$ scaling. We measure predictive sampling times over strata with $m\approx 512$--$576$ sampled rows; equality to baselines is covered in ALGO-002.\\
\begin{center}
\IfFileExists{../../results/tests/test003/PerfLarge.tex}{\input{../../results/tests/test003/PerfLarge.tex}}{}
\end{center}
\textbf{Scientific Analysis:}
- \emph{Advantages}: Predictive evaluation scales with connected in-degree and sampled rows, enabling tractable, deterministic timing at large $n$ while preserving exact equality (per ALGO-002). Sampling strata emphasise extremes and mid-band, aligning with index-set constructions and sensitivity analyses.
- \emph{Disadvantages}: Omission of exhaustive baselines at large $n$ precludes direct full-matrix timing comparisons; sample counts must be documented and kept deterministic for reproducibility; over-interpretation of sampling timings should be avoided (they reflect chosen $m$, not full $2^n$ enumeration).
- \emph{Net Impact}: The extreme values confirm that exhaustive baselines are impractical beyond $n\approx 13$, reinforcing the mechanism-first strategy: canonical index sets and importance sampling deliver exact equivalence with substantial performance benefits and manageable memory footprints.


\section{TEST-004: Acceptance Tests to Reproduce Manuscript Figures}
\textbf{Objective:} Consolidate and reproduce key manuscript figures and tables deterministically from generated artefacts, and summarise acceptance status across tickets.\\
\textbf{Acceptance Summary:}\\
\begin{center}
\begin{tabular}{lc}
\toprule
Ticket & Status \\
\midrule
TEST-001 & OK \\
TEST-002 & OK \\
TEST-003 & OK \\
\bottomrule
\end{tabular}
\end{center}
\subsection*{Summary of Experimental Results (Numeric)}
\textbf{Small Sizes ($n\leq 13$):} Per-seed timings and medians from artefacts.\\
\begin{center}
\IfFileExists{../../results/tests/test003/SeedsSmall.tex}{\input{../../results/tests/test003/SeedsSmall.tex}}{}
\end{center}
\begin{center}
\IfFileExists{../../results/tests/test003/PerfSmall.tex}{\input{../../results/tests/test003/PerfSmall.tex}}{}
\end{center}
\textbf{Large Sizes ($n\in\{20,50\}$):} Predictive sampling timings and sample counts.\\
\begin{center}
\IfFileExists{../../results/tests/test003/PerfLarge.tex}{\input{../../results/tests/test003/PerfLarge.tex}}{}
\end{center}
\subsection*{Scientific Analysis and Impact}
Observed timings for exhaustive baselines increase with $2^n$, confirming that full enumeration is practical up to $n\approx 13$. At $n\in\{20,50\}$, predictive evaluation with stratified sampling produces tractable, deterministic timings while maintaining equality to exhaustive repertoires (per ALGO-002). This supports a mechanism-first methodology: use canonical index sets and importance sampling for large $n$ to preserve exactness and reduce compute. Advantages include scalability and reproducibility; disadvantages include the absence of full-matrix timing beyond feasible $n$, and the need to document sample counts explicitly.

\section{EXPER-001: Ensembles for ER, Scale-Free and Small-World Graphs}
\textbf{Objective:} Generate deterministic ensembles for canonical random-network families (ER, Barabasi–Albert scale-free, Watts–Strogatz small-world), summarise structural metrics, and analyse implications for repertoire generation and predictive evaluation.\\
\textbf{Setup:} Sizes $n\in\{50,100\}$; seeds $\{401,402\}$; parameters: ER $p=0.05$ (expected degree $\approx pn$), scale-free $m=2$ edges per added node, small-world $k=4$ nearest neighbours with rewiring probability $p=0.05$. Metrics: average degree, global clustering (transitivity), mean shortest path on largest component, diameter.\\
\textbf{Results (per seed):}\\
\begin{center}
\IfFileExists{../../results/tests/exper001/Summary.tex}{\input{../../results/tests/exper001/Summary.tex}}{}
\end{center}
\textbf{Aggregated Across Seeds (medians with $\sigma$):}\\
\begin{center}
\IfFileExists{../../results/tests/exper001/SummaryAgg.tex}{\input{../../results/tests/exper001/SummaryAgg.tex}}{}
\end{center}
\textbf{Extended Metrics (assortativity and betweenness):}\\
\begin{center}
\IfFileExists{../../results/tests/exper001/SummaryAggExt.tex}{\input{../../results/tests/exper001/SummaryAggExt.tex}}{}
\end{center}
\textbf{Scientific Analysis:}
- ER exhibits low clustering and short mean paths as $n$ grows (random connectivity); diameters reduce correspondingly. Predictive evaluation benefits from sparsity at moderate $p$.
- Scale-free shows heavy-tailed degrees with moderate clustering; mean paths remain short (hub structure), diameters small. Repertoires concentrate around hub-influenced dynamics; importance sampling across Hamming strata and hub neighbourhoods is efficient.
- Small-world presents high clustering with relatively short paths (rewiring introduces shortcuts). The combination of local clustering and occasional long-range edges motivates mixed sampling: local neighbourhood strata plus random shortcuts.
- Seed variability is small for ER and moderate for small-world diameters due to rewiring-induced shortcuts; scale-free variability reflects hub placement randomness. Aggregated medians stabilise trends across $n\in\{50,100,200\}$.
 - Extended metrics: ER shows near-zero assortativity and moderate betweenness spread; scale-free exhibits positive assortativity from hubs and heavy-tailed betweenness; small-world shows near-zero assortativity with increasing betweenness as $n$ grows due to clustered local structure plus shortcuts. These profiles guide predictive sampling strategies: hub-focused and neighbourhood strata for scale-free, local clusters plus shortcut-aware strata for small-world, and uniform strata for ER.
\textbf{Impact on Research:} These ensembles provide controlled structural regimes that stress-test our predictive methods. High clustering (small-world) introduces local redundancy favouring canonical index-set composition; hubs (scale-free) amplify gate influence and sensitivity in predictive reconstructions; random sparsity (ER) aligns with low-cost baselines. Across models, predictive sampling remains appropriate at large $n$ while preserving equality to exhaustive repertoires in sampled validations.

\section{References}
\begin{thebibliography}{9}
\bibitem{OrderingMap} Bit-reversal mapping $\phi(j,n)=1+\mathrm{binrev}_n(j-1)$ used for MSB/LSB reconciliation; canonical in indexing literature.
\bibitem{ParityNoise} Parity under independent Bernoulli flips: $p_{\mathrm{flip}}(q,k)=\frac{1-(1-2q)^k}{2}$; derivable from binomial odd-count probabilities.
\bibitem{Booktabs} Documentation and usage of booktabs for high-quality tables in LaTeX.
\bibitem{Wolfram} Wolfram Language for deterministic gate truth tables and experimental artefact generation.
\end{thebibliography}


\section{EXPER-002: Gate Mixture Sweeps and Bias Control}
\textbf{Objective:} Quantify how mixtures of canonical gates control output bias given input bias $p$, and analyse stability via slopes at operating points.\\
\textbf{Setup:} Binary gates with analytical output bias functions. Mixtures formed as convex combinations of two gates with weight $w\in[0,1]$. Evaluations at $p\in\{0,0.1,\dots,1\}$; summary at $p=0.5$.\\
\textbf{Summary (at $p=0.5$):}\\
\begin{center}
\IfFileExists{../../results/tests/exper002/Summary.tex}{\input{../../results/tests/exper002/Summary.tex}}{}
\end{center}
\textbf{Sweeps ($p\mapsto p'$) for representative $w$):}\\
\begin{center}
\IfFileExists{../../results/tests/exper002/Sweep_AND_OR.tex}{\input{../../results/tests/exper002/Sweep_AND_OR.tex}}{}
\end{center}
\begin{center}
\IfFileExists{../../results/tests/exper002/Sweep_XOR_OR.tex}{\input{../../results/tests/exper002/Sweep_XOR_OR.tex}}{}
\end{center}
\begin{center}
\IfFileExists{../../results/tests/exper002/Sweep_AND_XOR.tex}{\input{../../results/tests/exper002/Sweep_AND_XOR.tex}}{}
\end{center}
\textbf{Scientific Analysis:}
- Mixtures of monotone gates (AND/OR) can realise a continuum of output biases from convex combinations; slopes indicate stability around $p=0.5$.
- XOR components flatten bias away from extremes, with slope reductions that stabilise neutral regimes; XNOR components increase symmetry and can restore neutrality.
- Non-monotone pairs (NAND/NOR) invert bias regions; weights tune bias reversal and can achieve target $p'$ precisely at $p=0.5$ for specific $w$.
\textbf{Impact:} Mixture selection provides a simple control mechanism to target desired bias and stability. For predictive evaluation, neutralising or amplifying bias can be achieved by tuning $w$, and slopes inform convergence vs sensitivity under iterative updates.

\textbf{Rationale:}
- We use representative pairs that span key bias behaviours under i.i.d. inputs: monotone (AND/OR), neutralising/symmetric (XOR/XNOR), and inverting (NAND/NOR).
- Many 2-input Boolean gates are redundant up to input/output negation or permutation; including all does not add new bias/slope regimes and increases clutter.
- Mixtures serve as control knobs; a small, interpretable basis makes tuning bias and stability clear without sacrificing useful behaviours.

\textbf{Behavioral Coverage:}
- Monotone gates: AND yields $p' = p^2$; OR yields $p' = 2p - p^2$. At $p=0.5$, both have positive slope, moving away from neutrality in a stable, monotone way.
- Symmetric/neutralisers: XOR yields $p' = 2p(1-p)$ with slope $0$ at $p=0.5$, flattening near neutrality; XNOR mirrors symmetry and helps restore neutrality.
- Inverters: NAND, NOR flip monotone behaviour (negative slopes at $p=0.5$), useful for bias reversal control.
- Unary NOT changes bias linearly and constants add no tuning value, so they are less informative for mixture sweeps.

\textbf{Intuition:}
- For i.i.d. inputs, 2-input gate output bias is a quadratic in $p$. The chosen set provides a compact basis of curve shapes: convex-up (AND), convex-down (OR), symmetric peak at $p=0.5$ (XOR), symmetric valley (XNOR), and sign-flipped variants (NAND/NOR).
- Mixing two gates is a convex combination of their bias curves, so with a handful of distinct shapes one can steer to target bias and control local slope (stability) at operating points.
- Practically, fewer well-chosen pairs keep tables readable and the control policy intuitive while covering the space of useful behaviours.




\section{EXPER-003: Update Regime Comparison (Sync vs Async)}
\textbf{Objective:} Compare convergence and attractor statistics between synchronous and asynchronous update regimes on canonical graph ensembles.\\
\textbf{Setup:} OR update per node using incoming neighbours; synchronous updates apply all nodes simultaneously; asynchronous updates randomly select a single node per step. Initial states are Bernoulli($p=0.5$).\\
\textbf{Summary:}\\
\begin{center}
\IfFileExists{../../results/tests/exper003/Summary.tex}{\input{../../results/tests/exper003/Summary.tex}}{}
\end{center}
\textbf{Analysis:}
- Synchronous updates rapidly reach fixed points under OR dynamics due to monotone expansion of ones; asynchronous single-node updates progress more slowly as information propagates locally.
- Cycle lengths are short in these regimes; most trajectories terminate in fixed points in synchronous mode; asynchronous mode shows slow approach with occasional short transients.
- Practical implication: evaluation strategies that assume synchronous composition can be optimistic; asynchronous schedules may need more iterations or stratified sampling to assess convergence and stability.



\section{EXPER-004: Subsystem Search Evaluation}
\textbf{Objective:} Evaluate a simple subsystem search heuristic over ER, scale-free and small-world ensembles, and quantify blockiness via cut fractions and compression factorisation.\\
\textbf{Heuristic:} Build undirected adjacency from the connectivity matrix and extract blocks via BFS over connected components. Compression computed per node (gate-dependent weight) and compared whole vs block-sum.\\
\textbf{Summary:}\\
\begin{center}
\IfFileExists{../../results/tests/exper004/Summary.tex}{\input{../../results/tests/exper004/Summary.tex}}{}
\end{center}
\textbf{Analysis:}
- On these ensembles the BFS heuristic yields a single block (the largest component), which is expected since connectivity is high; inter-block edges are absent, so cut fraction is $0$ and compression factorises.
- This provides a baseline: if block counts are $>1$ and cut fractions $>0$, the heuristic surfaces structural separations. For predictive strategies, high cut fractions signal cross-subsystem interactions that break factorisation assumptions.
- Extensions: replace raw connectivity with input-set similarity (Jaccard) or thresholded adjacency to split loosely connected regions; this can reveal finer subsystems and produce non-zero cut fractions for nuanced evaluation.


\section{EXPER-005: Noise Robustness Experiments}
\textbf{Objective:} Quantify gate-level robustness to independent input bit-flip noise with rate $q$, under unbiased inputs.\\
\textbf{Method:} Exact enumeration over inputs and noise patterns (no Monte Carlo), reporting the probability that noisy outputs equal noiseless outputs.\\
\textbf{Gate Robustness:}\\
\begin{center}
\IfFileExists{../../results/tests/exper005/GateNoise.tex}{\input{../../results/tests/exper005/GateNoise.tex}}{}
\end{center}
\textbf{Analysis:}
- Monotone gates (AND/OR) retain outputs with high probability for small $q$, as flips must jointly align to change outcomes.
- Parity gates (XOR/XNOR) are more sensitive since any odd number of input flips toggles the output, lowering correct rates.
- Majority is intermediate: single flips often do not change outcomes, but multiple flips can; correctness decays sublinearly with $q$ for small arities.
- Unary NOT simply mirrors input; correctness matches single-bit flip survival.\\
\textbf{Implications:} Systems that rely on parity-like logic are more susceptible to input noise; monotone and majority structures provide passive robustness. Mixing gates (cf. EXPER-002) can tune noise tolerance: adding majority/monotone components improves resilience at the cost of reduced sensitivity.

\textbf{Deeper Analysis:}
- Model: inputs flip independently with rate $q$ before evaluation. With $\pm 1$ coding and $\rho = 1-2q$, correctness is $(1+\mathrm{Stab}_\rho(f))/2$, where $\mathrm{Stab}_\rho(f)$ is the noise stability of $f$.
- First-order behaviour: for small $q$, $\Pr[\text{error}] \approx \mathrm{Inf}(f)\,q$ where $\mathrm{Inf}(f)$ is total influence (probability an input flip changes output under unbiased inputs).
- Gate-specific: AND/OR have $\mathrm{Inf}(f)=1$ (each input matters with probability $1/2$), yielding $\Pr[\text{correct}]\approx 1-q$. XOR/XNOR have $\Pr[\text{correct}] = (1+\rho^2)/2 = 1-2q(1-q)$. MAJORITY (3-input) has total influence $\approx 3\cdot 1/2 = 1.5$, so $\Pr[\text{correct}]\approx 1-1.5q + O(q^2)$. NOT gives $1-q$ exactly.
- Scaling: parity across $k$ inputs yields $(1+\rho^k)/2$, decaying faster with $k$; threshold/monotone gates degrade roughly linearly in $q$, with coefficients tied to boundary frequency.
- Design implications: mix parity with majority/monotone to trade sensitivity for robustness; bound parity arity to limit $\rho^k$ decay. Under synchronous monotone updates (cf. EXPER-003), perturbations damp quickly; asynchronous schedules propagate locally and extend transients. Structural blocks (cf. EXPER-004) confine noise when cut fractions are low; high inter-block coupling increases global sensitivity.

\section{TSK-COMPARE-002: PID-based Synergy Comparator}
\textbf{Objective:} Compute PID terms (synergy, unique, redundancy) for canonical 2-input gates under unbiased inputs, and analyse implications for mechanism-first design.\\
\textbf{Method:} Exact joint distribution $p(x_1,x_2,y)$ from gate truth tables; mutual informations $I(x_i;y)$ and $I(x_1,x_2;y)$; redundancy via $I_{\min}$ as $\sum_y p(y)\min\{I_{\mathrm{spec}}(x_1;y),I_{\mathrm{spec}}(x_2;y)\}$; unique $= I(x_i;y)-\mathrm{red}$; synergy $= I(x_1,x_2;y)-\mathrm{red}-\mathrm{uniq}_1-\mathrm{uniq}_2$.\\
\textbf{Summary:}\\
\begin{center}
\IfFileExists{../../results/tests/compare002/Summary.tex}{\input{../../results/tests/compare002/Summary.tex}}{}
\end{center}
\textbf{Analysis:}
- XOR/XNOR exhibit pure synergy of $1$ bit under unbiased inputs: $I(x_i;y)=0$, $I(x_1,x_2;y)=1$. This confirms that parity logic yields information only jointly, with no unique or redundant contributions.
- Monotone gates (AND/OR/NAND/NOR) show non-zero unique information from each input and a smaller positive synergy; redundancy is negligible under unbiased inputs for the $I_{\min}$ comparator. This reflects that each input carries partial information while joint knowledge resolves the output more decisively.
- Design implication: parity components maximise synergistic information and sensitivity; monotone components provide unique information and passive robustness. Mixed designs trade sensitivity (synergy) against robustness, in line with EXPER-005 noise analysis and EXPER-002 mixture control.



\section{TSK-COMPARE-003: Transfer Entropy and Multi-Information}
\textbf{Objective:} Evaluate transfer entropy $TE$ and total correlation $TC$ for canonical two-input gates under unbiased inputs, using the update $y_{t+1}=f(x_t,y_t)$.\\
\textbf{Method:} Exact distributions $p(x_t,y_t,y_{t+1})$ from truth tables; report $TE_{X\to Y}=I(x_t;y_{t+1}\mid y_t)$, $TE_{Y\to Y}=I(y_t;y_{t+1}\mid x_t)$ and $TC(X,Y,Y_{t+1})=\sum H(\cdot)-H(X,Y,Y_{t+1})$. For deterministic maps, $TC=H(y_{t+1})$.\\
\textbf{Summary:}\\
\begin{center}
\IfFileExists{../../results/tests/compare003/Summary.tex}{\input{../../results/tests/compare003/Summary.tex}}{}
\end{center}
\textbf{Analysis:}
- Parity gates (XOR/XNOR) maximise directed information flow and joint structure: $TE_{X\to Y}=TE_{Y\to Y}=1$ and $TC=1$. Information is purely synergistic, consistent with COMPARE‑002.
- Monotone gates (AND/OR/NAND/NOR) show $TE_{X\to Y}=TE_{Y\to Y}=0.5$ and $TC\approx0.811$. Each input contributes unique information with modest directed flow; joint structure is sub‑maximal.
- Implications: TE complements PID by quantifying directed influence conditioned on the other input; $TC$ summarises global coupling. Designs mixing parity and monotone elements tune both the amount and directionality of information flow in line with robustness–sensitivity trade‑offs (cf. EXPER‑005).


\end{document}